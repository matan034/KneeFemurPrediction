{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matan034/KneeFemurPrediction/blob/main/KneeProject_predict_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylZAVlS5lzLZ"
      },
      "source": [
        "#Predict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN6diklpPJuz"
      },
      "outputs": [],
      "source": [
        "# install required packages\n",
        "!pip install scikit-image\n",
        "!pip install numpy-stl\n",
        "!pip install trimesh\n",
        "!pip install pynrrd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "M9XGtR_olxU2"
      },
      "outputs": [],
      "source": [
        "import os,glob\n",
        "from google.colab import drive, files\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pickle\n",
        "import nibabel as nib\n",
        "from skimage import measure\n",
        "from stl import mesh\n",
        "import shutil\n",
        "import trimesh\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQU3Y3hFl9HB"
      },
      "outputs": [],
      "source": [
        "\n",
        "IMAGE_HEIGHT =  256\n",
        "IMAGE_WIDTH = 256\n",
        "IMG_SIZE = (IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "SCALE_FACTOR = 0.3\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "\n",
        "TRANSFER_LEARNING = True  # use attention unet with transfer learning\n",
        "CLASSIFIER = True         # use attention unet with a classifer\n",
        "EXTRA_LAYER = True        # use attention unet with extra layer\n",
        "USE_NII_FROM_DRIVE = True\n",
        "json_model_path = '/content/drive/MyDrive/KneeProject/Best results/Attention with everything/all_Attention-UNET_(256_256)_epochs:150_learning:0.0001_loss:Tver_batch:64.json'\n",
        "h5_model_path = '/content/drive/MyDrive/KneeProject/Best results/Attention with everything/all_Attention-UNET_(256_256)_epochs:150_learning:0.0001_loss:Tver_batch:64.h5'\n",
        "if USE_NII_FROM_DRIVE:\n",
        "  filename = '/content/drive/MyDrive/lab 1 texts/RF(Danny).nii'\n",
        "else:\n",
        "  uploaded = files.upload()\n",
        "  for file in uploaded:\n",
        "    filename = file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model architecture from JSON file\n",
        "with open(json_model_path, 'r') as json_file:\n",
        "    loaded_model_json = json_file.read()\n",
        "    loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
        "\n",
        "# Load weights into loaded model\n",
        "loaded_model.load_weights(h5_model_path)\n",
        "\n",
        "model = loaded_model\n",
        "\n",
        "\n",
        "print('Model loaded from disk.')"
      ],
      "metadata": {
        "id": "S5nSNcqDeXbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOLUME_SLICE_X=True   # use X axis in prediction\n",
        "VOLUME_SLICE_Y=True   # use Y axis in prediction\n",
        "VOLUME_SLICE_Z=True   # use Z axis in prediction\n",
        "HOUNSFIELD_MIN = -1000\n",
        "HOUNSFIELD_MAX = 2000\n",
        "HOUNSFIELD_RANGE = HOUNSFIELD_MAX - HOUNSFIELD_MIN\n",
        "\n",
        "def scaleImg(img, height, width):\n",
        "    return cv2.resize(img, dsize=(width, height), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "# Normalize image\n",
        "def normalizeImageIntensityRange(img):\n",
        "    img[img < HOUNSFIELD_MIN] = HOUNSFIELD_MIN\n",
        "    img[img > HOUNSFIELD_MAX] = HOUNSFIELD_MAX\n",
        "    return (img - HOUNSFIELD_MIN) / HOUNSFIELD_RANGE\n",
        "\n",
        "def predictVolume(inImg, toBin=True):\n",
        "    (xMax, yMax, zMax) = inImg.shape\n",
        "\n",
        "    outImgX = np.zeros((xMax, yMax, zMax))\n",
        "    outImgY = np.zeros((xMax, yMax, zMax))\n",
        "    outImgZ = np.zeros((xMax, yMax, zMax))\n",
        "\n",
        "    cnt = 0.0\n",
        "    if VOLUME_SLICE_X:\n",
        "        cnt += 1.0\n",
        "        for i in range(xMax):\n",
        "            img = inImg[i,:,:]\n",
        "            img = scaleImg(img, IMAGE_HEIGHT, IMAGE_WIDTH)[np.newaxis,:,:,np.newaxis]\n",
        "            if TRANSFER_LEARNING or CLASSIFIER:\n",
        "              img = np.repeat(img, 3, axis=3)\n",
        "            tmp = model.predict(img)[0,:,:,0]\n",
        "            outImgX[i,:,:] = scaleImg(tmp, yMax, zMax)\n",
        "    if VOLUME_SLICE_Y:\n",
        "        cnt += 1.0\n",
        "        for i in range(yMax):\n",
        "            img = scaleImg(inImg[:,i,:], IMAGE_HEIGHT, IMAGE_WIDTH)[np.newaxis,:,:,np.newaxis]\n",
        "            if TRANSFER_LEARNING or CLASSIFIER:\n",
        "              img = np.repeat(img, 3, axis=3)\n",
        "            tmp = model.predict(img)[0,:,:,0]\n",
        "            outImgY[:,i,:] = scaleImg(tmp, xMax, zMax)\n",
        "    if VOLUME_SLICE_Z:\n",
        "        cnt += 1.0\n",
        "        for i in range(zMax):\n",
        "            img = scaleImg(inImg[:,:,i], IMAGE_HEIGHT, IMAGE_WIDTH)[np.newaxis,:,:,np.newaxis]\n",
        "            if TRANSFER_LEARNING or CLASSIFIER:\n",
        "              img = np.repeat(img, 3, axis=3)\n",
        "            tmp = model.predict(img)[0,:,:,0]\n",
        "            outImgZ[:,:,i] = scaleImg(tmp, xMax, yMax)\n",
        "\n",
        "    outImg = (outImgX + outImgY + outImgZ)/cnt\n",
        "    if(toBin):\n",
        "        outImg[outImg>0.5] = 1.0\n",
        "        outImg[outImg<=0.5] = 0.0\n",
        "    return outImg"
      ],
      "metadata": {
        "id": "TMAE2ETKectg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgTargetNii = nib.load(filename)\n",
        "# Get the image data as a 3D array\n",
        "img_data = imgTargetNii.get_fdata()\n",
        "imgTarget = normalizeImageIntensityRange(img_data)\n",
        "predImg = predictVolume(imgTarget)"
      ],
      "metadata": {
        "id": "NoYWm51cla0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vertices, faces, _, _ = measure.marching_cubes(predImg)\n",
        "\n",
        "affine_matrix = imgTargetNii.affine\n",
        "\n",
        "\n",
        "# Add a homogeneous coordinate (1) to each vertex to make the matrix multiplication possible\n",
        "homogeneous_vertices = np.hstack((vertices, np.ones((len(vertices), 1))))\n",
        "\n",
        "# Apply the affine transformation\n",
        "vertices = np.dot(homogeneous_vertices, affine_matrix.T)[:, :3]  # Only keep the x, y, z coordinates\n",
        "reflection_matrix = np.array([[-1, 0, 0],\n",
        "                              [0, -1, 0],\n",
        "                              [0, 0, 1]])\n",
        "\n",
        "# Apply the reflection to the mesh vertices\n",
        "vertices = np.dot(vertices, reflection_matrix)\n",
        "knee_mesh=trimesh.Trimesh(vertices,faces)\n",
        "knee_mesh=trimesh.smoothing.filter_laplacian(knee_mesh,0.5,20)\n",
        "\n",
        "timestamp = datetime.now()\n",
        "mesh_path = f'knee-segmented{timestamp}.stl'\n",
        "knee_mesh.export(mesh_path)\n",
        "\n",
        "files.download(mesh_path)"
      ],
      "metadata": {
        "id": "u-gGPu7Ief4Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}